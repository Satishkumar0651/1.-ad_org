{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqkSjNc4jeNnvc5IlIzWPr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satishkumar0651/1.-ad_org/blob/master/Ecom_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/f85flvz4u9kyp9s/dataset_assignment.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIwGvsa5XsgG",
        "outputId": "770cddc5-bf70-4083-e22b-c310bd60f472"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-28 08:24:50--  https://www.dropbox.com/s/f85flvz4u9kyp9s/dataset_assignment.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/f85flvz4u9kyp9s/dataset_assignment.zip [following]\n",
            "--2023-07-28 08:24:51--  https://www.dropbox.com/s/raw/f85flvz4u9kyp9s/dataset_assignment.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com/cd/0/inline/CAsT_2OPwPNZfv6h8TpG2DTm8HMgKSys_3KlRtQfzQ4mTnivtlILHYUnXZd9GbGYY9Lpya3wpP2j-oSdP-t4G-lwat6k_OBnqR42hbYW4ZFnRdAXCs-IwnaaPHFUHbNn3OATbmtfLxs_wKwmm8XA2T5G/file# [following]\n",
            "--2023-07-28 08:24:51--  https://uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com/cd/0/inline/CAsT_2OPwPNZfv6h8TpG2DTm8HMgKSys_3KlRtQfzQ4mTnivtlILHYUnXZd9GbGYY9Lpya3wpP2j-oSdP-t4G-lwat6k_OBnqR42hbYW4ZFnRdAXCs-IwnaaPHFUHbNn3OATbmtfLxs_wKwmm8XA2T5G/file\n",
            "Resolving uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com (uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com (uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CAuRuOnJ7yyDoFdr5gnXf4ZDhpeugEnLbwBJ5nWT_8MWFbyh3tljOz7zMUoHqMXhGJ18c7CRkIBF5sC-VeGnw_fIRATby3vafnQeNbqkfxUrAJzGEBcrfPTlZnaBxxtcYfIVMbKVbk7gyuxQhCaX7dxACnHxFm5jaVsISDQUVIB3aD1t_6SGMleJJTH9jp3jQOTTuK2DEzOEzN7AtLe8NhaaEiMuwDbT7N2pxV-OsYwlDaQKV4ZI8H1gKSHRF4aGilNRUW4HWJ5OgLQuLSk_-ojHC_sTiqVbw99vOh2lpcbkA2uupdZnNQTmdz3d3ByLb991v98-F_YXUYbCMJTva_TuO5zaruACcnEZosNpIfVhPObMPv-H4_lY-ZN-UARElfY/file [following]\n",
            "--2023-07-28 08:24:51--  https://uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com/cd/0/inline2/CAuRuOnJ7yyDoFdr5gnXf4ZDhpeugEnLbwBJ5nWT_8MWFbyh3tljOz7zMUoHqMXhGJ18c7CRkIBF5sC-VeGnw_fIRATby3vafnQeNbqkfxUrAJzGEBcrfPTlZnaBxxtcYfIVMbKVbk7gyuxQhCaX7dxACnHxFm5jaVsISDQUVIB3aD1t_6SGMleJJTH9jp3jQOTTuK2DEzOEzN7AtLe8NhaaEiMuwDbT7N2pxV-OsYwlDaQKV4ZI8H1gKSHRF4aGilNRUW4HWJ5OgLQuLSk_-ojHC_sTiqVbw99vOh2lpcbkA2uupdZnNQTmdz3d3ByLb991v98-F_YXUYbCMJTva_TuO5zaruACcnEZosNpIfVhPObMPv-H4_lY-ZN-UARElfY/file\n",
            "Reusing existing connection to uc6b64ee5a938cee5ff8ff116253.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2264350 (2.2M) [application/zip]\n",
            "Saving to: ‘dataset_assignment.zip.1’\n",
            "\n",
            "dataset_assignment. 100%[===================>]   2.16M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-07-28 08:24:52 (29.7 MB/s) - ‘dataset_assignment.zip.1’ saved [2264350/2264350]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dataset_assignment.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX7oSzOeXemG",
        "outputId": "48436482-4274-4a73-dde3-dd71ce45ab16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset_assignment.zip\n",
            "replace dataset_assignment/test_set_sequences? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls dataset_assignment/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXkvbAffXpF-",
        "outputId": "7e32fe3b-bc15-4b49-9fd8-963ad238596f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stats  test_set_sequences  train_set_sequences\tval_set_sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next Movie Prediction**"
      ],
      "metadata": {
        "id": "9MldioeGMTrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "class RNNCCEModel(tf.keras.Model):\n",
        "    def __init__(self, num_items, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(num_items, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.GRU(rnn_units, return_sequences=False)\n",
        "        self.final_layer = tf.keras.layers.Dense(num_items)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Read the data from the file\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(lines, num_movies):\n",
        "    user_sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        user_id = int(data[0])\n",
        "        movie_ids = [int(data[i]) for i in range(1, len(data), 2)]\n",
        "\n",
        "        # Append each user sequence with movie IDs (except the last one) to user_sequences\n",
        "        user_sequences.append(movie_ids[:-1])\n",
        "\n",
        "        # Append the last movie ID in each user sequence to targets\n",
        "        targets.append(movie_ids[-1])\n",
        "\n",
        "    # Pad the sequences so they all have the same length\n",
        "    user_sequences = pad_sequences(user_sequences)\n",
        "\n",
        "    # Convert the lists to numpy arrays for TensorFlow\n",
        "    user_sequences = np.array(user_sequences)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    # Create one-hot encoded targets for Categorical Cross Entropy\n",
        "    targets = tf.keras.utils.to_categorical(targets, num_classes=num_movies)\n",
        "\n",
        "    return user_sequences, targets"
      ],
      "metadata": {
        "id": "6fG6whqBYNk3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the data from the files\n",
        "train_file_path = 'dataset_assignment/train_set_sequences'\n",
        "validation_file_path = 'dataset_assignment/val_set_sequences'\n",
        "test_file_path = 'dataset_assignment/test_set_sequences'\n",
        "\n",
        "train_lines = read_data(train_file_path)\n",
        "validation_lines = read_data(validation_file_path)\n",
        "test_lines = read_data(test_file_path)\n"
      ],
      "metadata": {
        "id": "fkEhtntfYXDc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBylq59kWYnh",
        "outputId": "7a0c2ce7-d402-42d4-a2f4-44faf3a066ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x78ace3349cf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x78ace3349cf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation SPS: 0.0000\n",
            "Epoch 2, Validation SPS: 0.0000\n",
            "Epoch 3, Validation SPS: 0.0000\n",
            "Epoch 4, Validation SPS: 0.0000\n",
            "Epoch 5, Validation SPS: 0.0000\n",
            "Epoch 6, Validation SPS: 0.0000\n",
            "Epoch 7, Validation SPS: 0.0000\n",
            "Epoch 8, Validation SPS: 0.0000\n",
            "Epoch 9, Validation SPS: 0.0000\n",
            "Epoch 10, Validation SPS: 0.0000\n",
            "Epoch 11, Validation SPS: 0.0000\n",
            "Epoch 12, Validation SPS: 0.0000\n",
            "Epoch 13, Validation SPS: 0.0000\n",
            "Epoch 14, Validation SPS: 0.0000\n",
            "Epoch 15, Validation SPS: 0.0000\n",
            "Epoch 16, Validation SPS: 0.0000\n",
            "Epoch 17, Validation SPS: 0.0000\n",
            "Epoch 18, Validation SPS: 0.0000\n",
            "Epoch 19, Validation SPS: 0.0000\n",
            "Epoch 20, Validation SPS: 0.0000\n",
            "Epoch 21, Validation SPS: 0.0000\n",
            "Epoch 22, Validation SPS: 0.0000\n",
            "Epoch 23, Validation SPS: 0.0000\n",
            "Epoch 24, Validation SPS: 0.0000\n",
            "Epoch 25, Validation SPS: 0.0000\n",
            "Epoch 26, Validation SPS: 0.0000\n",
            "Epoch 27, Validation SPS: 0.0000\n",
            "Epoch 28, Validation SPS: 0.0000\n",
            "Epoch 29, Validation SPS: 0.0000\n",
            "Epoch 30, Validation SPS: 0.0000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Assuming we have the number of unique movies (num_movies) from the dataset\n",
        "num_movies = 3706\n",
        "\n",
        "# Preprocess the data for training, validation, and test sets\n",
        "train_user_sequences, train_targets = preprocess_data(train_lines, num_movies)\n",
        "validation_user_sequences, validation_targets = preprocess_data(validation_lines, num_movies)\n",
        "test_user_sequences, test_targets = preprocess_data(test_lines, num_movies)\n",
        "\n",
        "\n",
        "# Create the training, validation, and test datasets using tf.data.Dataset\n",
        "batch_size = 32\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_user_sequences, train_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_user_sequences, validation_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_user_sequences, test_targets)).batch(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "rnn_units = 128\n",
        "num_epochs =  30\n",
        "batch_sizes =  64\n",
        "learning_rate =  0.01\n",
        "\n",
        "\n",
        "\n",
        "def compute_sps(model, dataset):\n",
        "    total_users = 0\n",
        "    total_sps = 0.0\n",
        "\n",
        "    for user_input, targets in dataset:\n",
        "        # The targets are one-hot encoded, so find the original movie_id of the first target\n",
        "        target_movie_id = tf.argmax(targets[:, 0])\n",
        "\n",
        "        #  model's predictions for the user\n",
        "        predictions = model(user_input)\n",
        "\n",
        "        # indices of the top-k predicted items for each user\n",
        "        # We're only interested in the first recommendation, so we set k=1\n",
        "        top_k_indices = tf.math.top_k(predictions, k=1).indices\n",
        "\n",
        "        # Check if the first target movie is in the top-k predictions\n",
        "        sps = tf.reduce_sum(tf.cast(tf.equal(target_movie_id, tf.cast(top_k_indices, tf.int64)), tf.float32))\n",
        "\n",
        "        total_sps += sps\n",
        "        total_users += 1\n",
        "\n",
        "    # Compute the average SPS over all users in the dataset\n",
        "    average_sps = total_sps / total_users\n",
        "\n",
        "    return average_sps.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Create the RNNCCE model\n",
        "model = RNNCCEModel(num_movies, embedding_dim, rnn_units)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for movie_input, targets in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(movie_input)\n",
        "            loss = loss_object(targets, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Evaluate the model on the validation set and compute the SPS metric\n",
        "    validation_sps = compute_sps(model, validation_dataset)\n",
        "    print(f\"Epoch {epoch + 1}, Validation SPS: {validation_sps:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next Three Movies Prediction**"
      ],
      "metadata": {
        "id": "qZPU0wi-Mf73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "class RNNCCEModel(tf.keras.Model):\n",
        "    def __init__(self, num_items, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(num_items, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.GRU(rnn_units, return_sequences=True)  # Now it returns the output for each timestep\n",
        "        self.final_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_items))  # Apply Dense layer to each timestep output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        # Select only the last three timesteps output for making predictions\n",
        "        x = x[:, -3:, :]\n",
        "        x = self.final_layer(x)\n",
        "        # Apply softmax over the final axis\n",
        "        x = tf.nn.softmax(x, axis=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Read the data from the .txt file\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return lines"
      ],
      "metadata": {
        "id": "wV8BpXFIrHYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(lines, num_movies):\n",
        "    user_sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        user_id = int(data[0])\n",
        "        movie_ids = [int(data[i]) for i in range(1, len(data), 2)]\n",
        "\n",
        "        # Append each user sequence with movie IDs (except the last three) to user_sequences\n",
        "        user_sequences.append(movie_ids[:-3])\n",
        "\n",
        "        # Append the last three movie IDs in each user sequence to targets\n",
        "        targets.append(movie_ids[-3:])\n",
        "\n",
        "    # Pad the sequences so they all have the same length\n",
        "    user_sequences = pad_sequences(user_sequences)\n",
        "    targets = pad_sequences(targets)\n",
        "\n",
        "    # Convert the lists to numpy arrays for TensorFlow\n",
        "    user_sequences = np.array(user_sequences)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    # Create one-hot encoded targets for Categorical Cross Entropy\n",
        "    targets = tf.keras.utils.to_categorical(targets, num_classes=num_movies)\n",
        "\n",
        "    return user_sequences, targets\n",
        "\n",
        "# Load the data from the files\n",
        "train_file_path = 'dataset_assignment/train_set_sequences'\n",
        "validation_file_path = 'dataset_assignment/val_set_sequences'\n",
        "test_file_path = 'dataset_assignment/test_set_sequences'\n",
        "\n",
        "train_lines = read_data(train_file_path)\n",
        "validation_lines = read_data(validation_file_path)\n",
        "test_lines = read_data(test_file_path)\n",
        "\n",
        "# Assuming we have the number of unique movies (num_movies) from the dataset\n",
        "num_movies = 3706\n",
        "\n",
        "# Preprocess the data for training, validation, and test sets\n",
        "train_user_sequences, train_targets = preprocess_data(train_lines, num_movies)\n",
        "validation_user_sequences, validation_targets = preprocess_data(validation_lines, num_movies)\n",
        "test_user_sequences, test_targets = preprocess_data(test_lines, num_movies)\n",
        "\n",
        "\n",
        "# Create the training, validation, and test datasets using tf.data.Dataset\n",
        "batch_size = 32\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_user_sequences, train_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_user_sequences, validation_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_user_sequences, test_targets)).batch(batch_size)\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for grid search\n",
        "embedding_dim = 100\n",
        "rnn_units = 128\n",
        "num_epochs =  30\n",
        "batch_sizes =  64\n",
        "learning_rate =  0.01\n",
        "\n",
        "def compute_sps(model, dataset):\n",
        "    total_users = 0\n",
        "    total_sps = 0.0\n",
        "\n",
        "    for user_input, targets in dataset:\n",
        "        # The targets are one-hot encoded, so find the original movie_id of the last three targets\n",
        "        target_movie_ids = tf.argmax(targets, axis=-1)  # shape: (batch_size, 3)\n",
        "\n",
        "        # model's predictions for the user\n",
        "        predictions = model(user_input)\n",
        "\n",
        "        # indices of the top-3 predicted items for each user at each timestep\n",
        "        top_k_indices = tf.math.top_k(predictions, k=3).indices  # shape: (batch_size, 3, 3)\n",
        "\n",
        "        # Check if the target movie is in the top-k predictions at the corresponding timestep\n",
        "        sps = tf.reduce_any(tf.equal(tf.expand_dims(target_movie_ids, axis=-1), tf.cast(top_k_indices, tf.int64)), axis=-1)  # shape: (batch_size, 3)\n",
        "        sps = tf.cast(sps, tf.float32)\n",
        "\n",
        "        total_sps += tf.reduce_sum(sps)  # Sum over the batch dimension\n",
        "        total_users += user_input.shape[0]  # Add the batch size to the total number of users\n",
        "\n",
        "    # Compute the average SPS over all users in the dataset\n",
        "    average_sps = total_sps / total_users\n",
        "\n",
        "    return average_sps.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Create the RNNCCE model\n",
        "model = RNNCCEModel(num_movies, embedding_dim, rnn_units)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for movie_input, targets in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(movie_input)\n",
        "            loss = loss_object(targets, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Evaluate the model on the validation set and compute the SPS metric\n",
        "    validation_sps = compute_sps(model, validation_dataset)\n",
        "    print(f\"Epoch {epoch + 1}, Validation SPS: {validation_sps:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ11h4AofV65",
        "outputId": "4d2a3939-4e1f-474c-b0d9-6de139bfaf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation SPS: 0.0500\n",
            "Epoch 2, Validation SPS: 0.0500\n",
            "Epoch 3, Validation SPS: 0.0500\n",
            "Epoch 4, Validation SPS: 0.0500\n",
            "Epoch 5, Validation SPS: 0.0500\n",
            "Epoch 6, Validation SPS: 0.0500\n",
            "Epoch 7, Validation SPS: 0.0500\n",
            "Epoch 8, Validation SPS: 0.0500\n",
            "Epoch 9, Validation SPS: 0.0500\n",
            "Epoch 10, Validation SPS: 0.0500\n",
            "Epoch 11, Validation SPS: 0.0500\n",
            "Epoch 12, Validation SPS: 0.0500\n",
            "Epoch 13, Validation SPS: 0.0500\n",
            "Epoch 14, Validation SPS: 0.0500\n",
            "Epoch 15, Validation SPS: 0.0500\n",
            "Epoch 16, Validation SPS: 0.0500\n",
            "Epoch 17, Validation SPS: 0.0500\n",
            "Epoch 18, Validation SPS: 0.0500\n",
            "Epoch 19, Validation SPS: 0.0500\n",
            "Epoch 20, Validation SPS: 0.0500\n",
            "Epoch 21, Validation SPS: 0.0500\n",
            "Epoch 22, Validation SPS: 0.0500\n",
            "Epoch 23, Validation SPS: 0.0500\n",
            "Epoch 24, Validation SPS: 0.0500\n",
            "Epoch 25, Validation SPS: 0.0500\n",
            "Epoch 26, Validation SPS: 0.0500\n",
            "Epoch 27, Validation SPS: 0.0500\n",
            "Epoch 28, Validation SPS: 0.0500\n",
            "Epoch 29, Validation SPS: 0.0500\n",
            "Epoch 30, Validation SPS: 0.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npF2wUNrzV3c",
        "outputId": "346c5651-2352-4e40-b5fb-b5dd22fbbceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.5-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/176.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.27.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.3.5 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "KjFBm10lMxnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "class RNNCCEModel(tf.keras.Model):\n",
        "    def __init__(self, num_items, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(num_items, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.GRU(rnn_units, return_sequences=True)\n",
        "        self.final_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_items))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        x = x[:, -3:, :]\n",
        "        x = self.final_layer(x)\n",
        "        x = tf.nn.softmax(x, axis=-1)\n",
        "        return x\n",
        "\n",
        "def model_builder(hp):\n",
        "    num_items = 3706\n",
        "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
        "    hp_embedding_dim = hp.Int('embedding_dim', min_value = 32, max_value = 512, step = 32)\n",
        "    model = RNNCCEModel(num_items, hp_embedding_dim, hp_units)\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
        "\n",
        "    model.compile(optimizer = optimizer, loss = loss_object)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KkKgydLzWgs",
        "outputId": "52595432-2f51-4b70-d40e-946940c9a371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-1a6bb2b7b096>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  import kerastuner as kt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective = 'loss',\n",
        "                     max_epochs = 10,\n",
        "                     factor = 3)\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n",
        "tuner.search(train_dataset, epochs = 50, validation_data = validation_dataset, callbacks = [stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFC0UhIfzbxS",
        "outputId": "80d39167-ce72-4fb0-d8c0-7a8af36aaaa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26 Complete [00h 04m 04s]\n",
            "loss: 6.835476398468018\n",
            "\n",
            "Best loss So Far: 4.947273254394531\n",
            "Total elapsed time: 00h 49m 10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CInvKbWuzjcp",
        "outputId": "29dae98c-f137-42c8-96d3-bff706b88382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_tuner.tuners.hyperband.Hyperband at 0x781580420610>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "print(f\"The optimal number of units is {best_hps.get('units')}, the optimal embedding dimension is {best_hps.get('embedding_dim')}, and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjVGRkHXAfvy",
        "outputId": "7cf66a34-9cf0-4eda-9f97-42f51f24796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal number of units is 256, the optimal embedding dimension is 416, and the optimal learning rate for the optimizer is 0.001.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_dim = 416\n",
        "rnn_units = 256\n",
        "num_epochs =  30\n",
        "batch_sizes =  64\n",
        "learning_rate =  0.001\n",
        "num_movies=3706\n",
        "# Create the RNNCCE model with optimal hyperparameters\n",
        "model = RNNCCEModel(num_items=3706, embedding_dim=416, rnn_units=256)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    for movie_input, targets in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(movie_input)\n",
        "            loss = loss_object(targets, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Evaluate the model on the validation set and compute the SPS metric\n",
        "    validation_sps = compute_sps(model, validation_dataset)\n",
        "    print(f\"Epoch {epoch + 1}, Validation SPS: {validation_sps:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3pjDEpSAqbx",
        "outputId": "4ca6ce08-1a3d-48fd-ed33-1d7a7b9ad481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation SPS: 0.0680\n",
            "Epoch 2, Validation SPS: 0.0480\n",
            "Epoch 3, Validation SPS: 0.0480\n",
            "Epoch 4, Validation SPS: 0.0480\n",
            "Epoch 5, Validation SPS: 0.0480\n",
            "Epoch 6, Validation SPS: 0.0480\n",
            "Epoch 7, Validation SPS: 0.0480\n",
            "Epoch 8, Validation SPS: 0.0480\n",
            "Epoch 9, Validation SPS: 0.0480\n",
            "Epoch 10, Validation SPS: 0.0640\n",
            "Epoch 11, Validation SPS: 0.0640\n",
            "Epoch 12, Validation SPS: 0.0640\n",
            "Epoch 13, Validation SPS: 0.0640\n",
            "Epoch 14, Validation SPS: 0.0640\n",
            "Epoch 15, Validation SPS: 0.0640\n",
            "Epoch 16, Validation SPS: 0.0640\n",
            "Epoch 17, Validation SPS: 0.0640\n",
            "Epoch 18, Validation SPS: 0.0640\n",
            "Epoch 19, Validation SPS: 0.0640\n",
            "Epoch 20, Validation SPS: 0.0640\n",
            "Epoch 21, Validation SPS: 0.0640\n",
            "Epoch 22, Validation SPS: 0.0640\n",
            "Epoch 23, Validation SPS: 0.0640\n",
            "Epoch 24, Validation SPS: 0.0640\n",
            "Epoch 25, Validation SPS: 0.0640\n",
            "Epoch 26, Validation SPS: 0.0640\n",
            "Epoch 27, Validation SPS: 0.0640\n",
            "Epoch 28, Validation SPS: 0.0640\n",
            "Epoch 29, Validation SPS: 0.0640\n",
            "Epoch 30, Validation SPS: 0.0640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPS on test_dataset**"
      ],
      "metadata": {
        "id": "iVE8MQByND4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on the test dataset\n",
        "test_sps = compute_sps(model, test_dataset)\n",
        "print(f\"Test SPS: {test_sps:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peKVTHP1DQQW",
        "outputId": "3d73c4d7-7a87-4557-f624-889344f4d3f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test SPS: 0.0380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# # This will prompt for authorization.\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adnqqg4nKaIN",
        "outputId": "244e3459-918d-4a82-fc80-223f7715544e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saved Model**"
      ],
      "metadata": {
        "id": "NJrLDJYtNM_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model's weights for later use\n",
        "model.save_weights('./dataset_assignment/RNNCCEModel_weights.h5')"
      ],
      "metadata": {
        "id": "A56OwquAJPXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_movies = 3706\n",
        "embedding_dim = 416\n",
        "rnn_units = 256\n",
        "model = RNNCCEModel(num_movies, embedding_dim, rnn_units)\n",
        "import numpy as np\n",
        "\n",
        "# Create a random tensor\n",
        "x = np.random.random((1, 10))\n",
        "\n",
        "# Call the model on the data\n",
        "model(x)\n",
        "model.load_weights('RNNCCEModel_weights.h5')"
      ],
      "metadata": {
        "id": "bPgsiRGSQhbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate SPS on test dataset\n",
        "test_sps = compute_sps(model, test_dataset)\n",
        "print(f\"SPS on test dataset: {test_sps:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7jdc-BlQhXZ",
        "outputId": "7175ba50-bdd1-46e9-859a-f25c3568bc44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPS on test dataset: 0.0380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7NydXNXKiXf"
      },
      "source": [
        "# QnA Section:\n",
        "Please answer the following questions based on your understanding of your paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOsBp4268jml"
      },
      "source": [
        "### Question 1:\n",
        "\n",
        "You have a set of 3 sequences:\n",
        "```\n",
        "seq1 = ['item1', 'item5', 'item3', 'item2', 'item4']\n",
        "seq2 = ['item10', 'item5', 'item4', 'item8', 'item2', 'item1', 'item3']\n",
        "seq3 = ['item8', 'item2', 'item5', 'item3', 'item2', 'item10']\n",
        "```\n",
        "\n",
        "We take the first 3 items from all these sequences, and feed them in a RNN model.\n",
        "The model generates outputs for 3 timesteps for all the 3 input sequences.\n",
        "Following are the model outputs, where out1 corresponds to the output for input sequence seq1, and so on-\n",
        "```\n",
        "out1 = ['item2', 'item4', 'item1']   \n",
        "out2 = ['item2', 'item10', 'item1']   \n",
        "out3 = ['item2', 'item3', 'item5']   \n",
        "```\n",
        "\n",
        "What will be the sps@3 of this RNN model, for the given set of input sequences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLv0u17n_CaS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For seq1, the actual following 3 items are ['item5', 'item3', 'item2'], but the model predicted ['item2', 'item4', 'item1']. None of the predicted items match the actual items. Hence, sps for seq1 is 0.\n",
        "\n",
        "For seq2, the actual following 3 items are ['item4', 'item8', 'item2'], but the model predicted ['item2', 'item10', 'item1']. Here, the first predicted item matches with the third actual item. Hence, sps for seq2 is 1/3.\n",
        "\n",
        "For seq3, the actual following 3 items are ['item5', 'item3', 'item2'], but the model predicted ['item2', 'item3', 'item5']. Here, the first and second predicted items match the third and second actual items, respectively. However, sps considers only whether the next item is in the recommendation list, not the order of items. Hence, sps for seq3 is 1.\n",
        "\n",
        "The average sps for these 3 sequences would then be (0 + 1/3 + 1)/3 = 0.44 (approximately)."
      ],
      "metadata": {
        "id": "elazh1e4mZxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_EIk1YVSmW1_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xrxxewi9zoT"
      },
      "source": [
        "### Question 2:\n",
        "\n",
        "Which do you think is the most significant metric for a recommendation system: sps or recall?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Z9xg1x_C9B"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both sps and recall are significant metrics for a recommendation system, and the choice depends on the context. SPS is a metric that emphasizes short-term, next-item prediction, which is crucial for applications like predicting the next click in a web browsing session. On the other hand, recall is a more general measure of how well the recommendation system can predict all the relevant items for a user, without focusing on the sequence in which they occur. Therefore, if sequence matters (e.g., in a playlist or browsing session), sps may be more significant, while in other contexts (like book or movie recommendation), recall might be more relevant."
      ],
      "metadata": {
        "id": "g7zJAUWom4va"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ2CQrYt83U5"
      },
      "source": [
        "### Question 3:\n",
        "\n",
        "How does the paper propose to measure the short term / long term profiling of a recommender system?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSX6XvTC_Dgd"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper proposes measuring the short-term and long-term profiling of a recommender system through a series of experiments involving distinct metrics. The short-term profiling can be assessed using metrics like sps (Short-term Prediction Success), which is a measure of the system's ability to predict the next item in a user's sequence.\n",
        "\n",
        "Long-term profiling, on the other hand, can be evaluated using metrics that measure the overall accuracy of recommendations over a more extended period, such as recall or precision. These metrics evaluate how well the system predicts all the relevant items for a user, regardless of their sequence."
      ],
      "metadata": {
        "id": "yw9Aw6z5nI3W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc7QQ5gn8K4Y"
      },
      "source": [
        "### Question 4:\n",
        "Suppose you have a user U who has watched 2*n movies, and you have trained RNN based recommender with first n movies. Now, you have a number N <= n, denoting the number of next items in the user sequence taken to do short-term/long-term profiling.\n",
        "As per the paper, what impact does increasing N have on this profiling?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPi84Ifd8Tnh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per the paper, increasing the value of N, which represents the number of next items in the user sequence used for short-term/long-term profiling, has several impacts:\n",
        "\n",
        "Short-term Profiling: When increasing N, the model gets a larger context of user's recent interactions, which could lead to more accurate next-item predictions. However, it's not always the case that a larger N would result in better short-term prediction success (SPS), as the model might start focusing on long-term preferences, thus missing out on capturing the immediate preferences. The optimal value of N for short-term profiling may depend on the specific characteristics of the data and the user's behavior.\n",
        "\n",
        "Long-term Profiling: Increasing N tends to favor long-term profiling as the model gets more information about the user's overall preferences rather than just their immediate or most recent ones. This could result in a better recall or precision at K (where K is the number of recommendations made), as the model can surface items that align with the user's broader taste.\n",
        "\n",
        "Remember, these impacts may vary depending on the specifics of the dataset, the users, the items, and the model used. In some cases, too large a context may introduce noise and reduce performance. Hence, careful experimentation and cross-validation are necessary to find the optimal value of N."
      ],
      "metadata": {
        "id": "Z-m5DOcLnuMz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTDImS18nwCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KIRquyUvJbrd",
        "outputId": "9e6b1042-a09f-4cfc-cc11-f385fc6c0c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4nQWjvXaK7cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKo5jZWRJsj5",
        "outputId": "100e04fb-9df7-413b-aea3-6850316a17d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdataset_assignment\u001b[0m/     \u001b[01;34m__MACOSX\u001b[0m/               \u001b[01;34msample_data\u001b[0m/\n",
            "dataset_assignment.zip  RNNCCEModel_weights.h5  \u001b[01;34muntitled_project\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "class RNNCCEModel(tf.keras.Model):\n",
        "    def __init__(self, num_items, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(num_items, embedding_dim)\n",
        "        self.rnn = tf.keras.layers.GRU(rnn_units, return_sequences=True)  # Now it returns the output for each timestep\n",
        "        self.final_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_items))  # Apply Dense layer to each timestep output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.rnn(x)\n",
        "        # Select only the last ten timesteps output for making predictions\n",
        "        x = x[:, -10:, :]\n",
        "        x = self.final_layer(x)\n",
        "        # Apply softmax over the final axis\n",
        "        x = tf.nn.softmax(x, axis=-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Read the data from the .txt file\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return lines\n",
        "\n",
        "# Read the data from the file\n",
        "def read_data(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return lines\n",
        "\n",
        "def preprocess_data(lines, num_movies):\n",
        "    sequence_length = 10  # Hard-coded sequence length\n",
        "    user_sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for line in lines:\n",
        "        data = line.strip().split()\n",
        "        user_id = int(data[0])\n",
        "        movie_ids = [int(data[i]) for i in range(1, len(data), 2)]\n",
        "\n",
        "        # Split each user sequence into input sequences and target sequences\n",
        "        for i in range(len(movie_ids) - sequence_length - 10):  # Changed the range here\n",
        "            input_seq = movie_ids[i:i+sequence_length]\n",
        "            target_seq = movie_ids[i+sequence_length:i+sequence_length+10]  # Get the next 10 movie IDs as targets\n",
        "\n",
        "            user_sequences.append(input_seq)\n",
        "            targets.append(target_seq)\n",
        "\n",
        "    # Pad the sequences so they all have the same length\n",
        "    user_sequences = pad_sequences(user_sequences)\n",
        "\n",
        "    # Convert the lists to numpy arrays for TensorFlow\n",
        "    user_sequences = np.array(user_sequences)\n",
        "\n",
        "    # Convert target sequences to one-hot encoded arrays\n",
        "    targets = np.array([tf.keras.utils.to_categorical(target_seq, num_classes=num_movies) for target_seq in targets])\n",
        "\n",
        "    return user_sequences, targets\n"
      ],
      "metadata": {
        "id": "WNqWaairm3zI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the data from the files\n",
        "train_file_path = 'dataset_assignment/train_set_sequences'\n",
        "validation_file_path = 'dataset_assignment/val_set_sequences'\n",
        "test_file_path = 'dataset_assignment/test_set_sequences'\n",
        "\n",
        "train_lines = read_data(train_file_path)\n",
        "validation_lines = read_data(validation_file_path)\n",
        "test_lines = read_data(test_file_path)\n",
        "\n",
        "# Assuming we have the number of unique movies (num_movies) from the dataset\n",
        "num_movies = 3706\n",
        "\n",
        "# Preprocess the data for training, validation, and test sets\n",
        "train_user_sequences, train_targets = preprocess_data(train_lines, num_movies)\n",
        "validation_user_sequences, validation_targets = preprocess_data(validation_lines, num_movies)\n",
        "test_user_sequences, test_targets = preprocess_data(test_lines, num_movies)"
      ],
      "metadata": {
        "id": "XJH9jeCNm_R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the training, validation, and test datasets using tf.data.Dataset\n",
        "batch_size = 32\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_user_sequences, train_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_user_sequences, validation_targets)).batch(batch_size)\n",
        "\n",
        "# Prepare the test dataset\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_user_sequences, test_targets)).batch(batch_size)\n",
        "\n",
        "\n",
        "# Define the hyperparameter grid for grid search\n",
        "embedding_dim = 100\n",
        "rnn_units = 128\n",
        "num_epochs =  30\n",
        "batch_sizes =  64\n",
        "learning_rate =  0.01\n",
        "\n",
        "def compute_sps(model, dataset):\n",
        "    total_users = 0\n",
        "    total_sps = 0.0\n",
        "\n",
        "    for user_input, targets in dataset:\n",
        "        # The targets are one-hot encoded, so find the original movie_id of the last three targets\n",
        "        target_movie_ids = tf.argmax(targets, axis=-1)  # shape: (batch_size, 3)\n",
        "\n",
        "        # model's predictions for the user\n",
        "        predictions = model(user_input)\n",
        "\n",
        "        # indices of the top-3 predicted items for each user at each timestep\n",
        "        top_k_indices = tf.math.top_k(predictions, k=10).indices  # shape: (batch_size, 3, 3)\n",
        "\n",
        "        # Check if the target movie is in the top-k predictions at the corresponding timestep\n",
        "        sps = tf.reduce_any(tf.equal(tf.expand_dims(target_movie_ids, axis=-1), tf.cast(top_k_indices, tf.int64)), axis=-1)  # shape: (batch_size, 3)\n",
        "        sps = tf.cast(sps, tf.float32)\n",
        "\n",
        "        total_sps += tf.reduce_sum(sps)  # Sum over the batch dimension\n",
        "        total_users += user_input.shape[0]  # Add the batch size to the total number of users\n",
        "\n",
        "    # Compute the average SPS over all users in the dataset\n",
        "    average_sps = total_sps / total_users\n",
        "\n",
        "    return average_sps.numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Create the RNNCCE model\n",
        "model = RNNCCEModel(num_movies, embedding_dim, rnn_units)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for movie_input, targets in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(movie_input)\n",
        "            loss = loss_object(targets, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Evaluate the model on the validation set and compute the SPS metric\n",
        "    validation_sps = compute_sps(model, validation_dataset)\n",
        "    print(f\"Epoch {epoch + 1}, Validation SPS: {validation_sps:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SQ0kPsfHJ0xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OAL7sgqkleRg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}